{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a entrenar una red para que sume 3 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Generamos training set con 1000 datos. \n",
    "# Input: vector de 3 números aleatorios X1, X2 y X3 (inputs) y los sumamos (Y, output) \n",
    "X1=np.random.uniform(size=1000)*100\n",
    "X2=np.random.uniform(size=1000)*100\n",
    "X3=np.random.uniform(size=1000)*100\n",
    "X=np.transpose([X1,X2,X3])\n",
    "# soluciones\n",
    "Y=X1+X2+X3  \n",
    "print np.shape(X),np.shape(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750,), (250,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y) \n",
    "# separa el set total entre train y test (3/4 y 1/4 por default) . Exactamente lo mismo que hacer:\n",
    "#ntrain=3*len(y)/4\n",
    "#X_train=X[:ntrain,:];y_train=y[:ntrain]\n",
    "#X_test=X[ntrain:,:];y_test=y[ntrain:]\n",
    "np.shape(y_train),np.shape(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# definimos numero de capas y de hidden nodes (3 capas, de 2 ,5  y 10 neuronas) \n",
    "# Exagerado para este problema ..\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,10,10),max_iter=500,verbose=True) # batch_size default depending on minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 13116.85425455\n",
      "Iteration 2, loss = 12781.25253028\n",
      "Iteration 3, loss = 12428.90432642\n",
      "Iteration 4, loss = 12044.45712259\n",
      "Iteration 5, loss = 11652.34443350\n",
      "Iteration 6, loss = 11254.03966685\n",
      "Iteration 7, loss = 10847.36174263\n",
      "Iteration 8, loss = 10450.12475153\n",
      "Iteration 9, loss = 10072.10527810\n",
      "Iteration 10, loss = 9705.26908473\n",
      "Iteration 11, loss = 9341.62483850\n",
      "Iteration 12, loss = 8968.40037166\n",
      "Iteration 13, loss = 8566.68826984\n",
      "Iteration 14, loss = 8148.23813639\n",
      "Iteration 15, loss = 7703.14347288\n",
      "Iteration 16, loss = 7225.57755423\n",
      "Iteration 17, loss = 6694.58481560\n",
      "Iteration 18, loss = 6096.30905146\n",
      "Iteration 19, loss = 5456.33020224\n",
      "Iteration 20, loss = 4779.01580025\n",
      "Iteration 21, loss = 4085.04140666\n",
      "Iteration 22, loss = 3384.95240466\n",
      "Iteration 23, loss = 2731.21568768\n",
      "Iteration 24, loss = 2101.58635059\n",
      "Iteration 25, loss = 1555.42797735\n",
      "Iteration 26, loss = 1073.69729198\n",
      "Iteration 27, loss = 702.68903330\n",
      "Iteration 28, loss = 420.40269998\n",
      "Iteration 29, loss = 238.57585953\n",
      "Iteration 30, loss = 133.00011817\n",
      "Iteration 31, loss = 86.47909840\n",
      "Iteration 32, loss = 72.52431202\n",
      "Iteration 33, loss = 72.36846672\n",
      "Iteration 34, loss = 73.70003912\n",
      "Iteration 35, loss = 70.81387579\n",
      "Iteration 36, loss = 64.35550379\n",
      "Iteration 37, loss = 55.96390543\n",
      "Iteration 38, loss = 47.73915209\n",
      "Iteration 39, loss = 41.49597504\n",
      "Iteration 40, loss = 36.78427254\n",
      "Iteration 41, loss = 33.41724880\n",
      "Iteration 42, loss = 30.97734403\n",
      "Iteration 43, loss = 28.68590376\n",
      "Iteration 44, loss = 26.54944712\n",
      "Iteration 45, loss = 24.60751802\n",
      "Iteration 46, loss = 22.67726034\n",
      "Iteration 47, loss = 20.97907100\n",
      "Iteration 48, loss = 19.39192045\n",
      "Iteration 49, loss = 18.00989622\n",
      "Iteration 50, loss = 16.72974804\n",
      "Iteration 51, loss = 15.62827521\n",
      "Iteration 52, loss = 14.56944657\n",
      "Iteration 53, loss = 13.59930456\n",
      "Iteration 54, loss = 12.75319289\n",
      "Iteration 55, loss = 11.94957699\n",
      "Iteration 56, loss = 11.20790512\n",
      "Iteration 57, loss = 10.52268976\n",
      "Iteration 58, loss = 9.93514954\n",
      "Iteration 59, loss = 9.37094351\n",
      "Iteration 60, loss = 8.84485350\n",
      "Iteration 61, loss = 8.38279140\n",
      "Iteration 62, loss = 7.94225376\n",
      "Iteration 63, loss = 7.54840407\n",
      "Iteration 64, loss = 7.19680840\n",
      "Iteration 65, loss = 6.86249079\n",
      "Iteration 66, loss = 6.56120249\n",
      "Iteration 67, loss = 6.29416942\n",
      "Iteration 68, loss = 6.03855045\n",
      "Iteration 69, loss = 5.80378827\n",
      "Iteration 70, loss = 5.59209331\n",
      "Iteration 71, loss = 5.40676779\n",
      "Iteration 72, loss = 5.22746711\n",
      "Iteration 73, loss = 5.06177983\n",
      "Iteration 74, loss = 4.90687726\n",
      "Iteration 75, loss = 4.77299224\n",
      "Iteration 76, loss = 4.64568034\n",
      "Iteration 77, loss = 4.52902891\n",
      "Iteration 78, loss = 4.41828831\n",
      "Iteration 79, loss = 4.32300484\n",
      "Iteration 80, loss = 4.23230248\n",
      "Iteration 81, loss = 4.14448347\n",
      "Iteration 82, loss = 4.07365408\n",
      "Iteration 83, loss = 3.99775360\n",
      "Iteration 84, loss = 3.93231627\n",
      "Iteration 85, loss = 3.87354949\n",
      "Iteration 86, loss = 3.81567335\n",
      "Iteration 87, loss = 3.76654912\n",
      "Iteration 88, loss = 3.71856383\n",
      "Iteration 89, loss = 3.66821232\n",
      "Iteration 90, loss = 3.62931332\n",
      "Iteration 91, loss = 3.58696818\n",
      "Iteration 92, loss = 3.55183687\n",
      "Iteration 93, loss = 3.51418483\n",
      "Iteration 94, loss = 3.48671274\n",
      "Iteration 95, loss = 3.45387639\n",
      "Iteration 96, loss = 3.42506736\n",
      "Iteration 97, loss = 3.39798216\n",
      "Iteration 98, loss = 3.37177472\n",
      "Iteration 99, loss = 3.34930685\n",
      "Iteration 100, loss = 3.32594802\n",
      "Iteration 101, loss = 3.30409483\n",
      "Iteration 102, loss = 3.28197641\n",
      "Iteration 103, loss = 3.26284189\n",
      "Iteration 104, loss = 3.24669050\n",
      "Iteration 105, loss = 3.22804412\n",
      "Iteration 106, loss = 3.21162114\n",
      "Iteration 107, loss = 3.19597237\n",
      "Iteration 108, loss = 3.17969832\n",
      "Iteration 109, loss = 3.16506034\n",
      "Iteration 110, loss = 3.15238158\n",
      "Iteration 111, loss = 3.13886315\n",
      "Iteration 112, loss = 3.12400924\n",
      "Iteration 113, loss = 3.11251710\n",
      "Iteration 114, loss = 3.10047036\n",
      "Iteration 115, loss = 3.08863343\n",
      "Iteration 116, loss = 3.07641053\n",
      "Iteration 117, loss = 3.06661509\n",
      "Iteration 118, loss = 3.05494579\n",
      "Iteration 119, loss = 3.04424928\n",
      "Iteration 120, loss = 3.03475029\n",
      "Iteration 121, loss = 3.02405980\n",
      "Iteration 122, loss = 3.01475522\n",
      "Iteration 123, loss = 3.00565147\n",
      "Iteration 124, loss = 2.99539759\n",
      "Iteration 125, loss = 2.98556668\n",
      "Iteration 126, loss = 2.97730447\n",
      "Iteration 127, loss = 2.96869878\n",
      "Iteration 128, loss = 2.95961900\n",
      "Iteration 129, loss = 2.95150798\n",
      "Iteration 130, loss = 2.94236597\n",
      "Iteration 131, loss = 2.93476485\n",
      "Iteration 132, loss = 2.92567773\n",
      "Iteration 133, loss = 2.91871061\n",
      "Iteration 134, loss = 2.91042021\n",
      "Iteration 135, loss = 2.90195851\n",
      "Iteration 136, loss = 2.89399739\n",
      "Iteration 137, loss = 2.88640203\n",
      "Iteration 138, loss = 2.87835346\n",
      "Iteration 139, loss = 2.87113969\n",
      "Iteration 140, loss = 2.86419620\n",
      "Iteration 141, loss = 2.85607368\n",
      "Iteration 142, loss = 2.84838861\n",
      "Iteration 143, loss = 2.84080920\n",
      "Iteration 144, loss = 2.83429833\n",
      "Iteration 145, loss = 2.82638451\n",
      "Iteration 146, loss = 2.81892348\n",
      "Iteration 147, loss = 2.81167699\n",
      "Iteration 148, loss = 2.80450268\n",
      "Iteration 149, loss = 2.79703585\n",
      "Iteration 150, loss = 2.79009208\n",
      "Iteration 151, loss = 2.78359983\n",
      "Iteration 152, loss = 2.77566007\n",
      "Iteration 153, loss = 2.76835742\n",
      "Iteration 154, loss = 2.76152145\n",
      "Iteration 155, loss = 2.75455567\n",
      "Iteration 156, loss = 2.74766239\n",
      "Iteration 157, loss = 2.74208779\n",
      "Iteration 158, loss = 2.73360028\n",
      "Iteration 159, loss = 2.72736109\n",
      "Iteration 160, loss = 2.72022579\n",
      "Iteration 161, loss = 2.71275613\n",
      "Iteration 162, loss = 2.70638144\n",
      "Iteration 163, loss = 2.69885650\n",
      "Iteration 164, loss = 2.69184961\n",
      "Iteration 165, loss = 2.68483884\n",
      "Iteration 166, loss = 2.67785847\n",
      "Iteration 167, loss = 2.67098526\n",
      "Iteration 168, loss = 2.66457409\n",
      "Iteration 169, loss = 2.65754211\n",
      "Iteration 170, loss = 2.65022833\n",
      "Iteration 171, loss = 2.64400654\n",
      "Iteration 172, loss = 2.63618130\n",
      "Iteration 173, loss = 2.62985394\n",
      "Iteration 174, loss = 2.62249878\n",
      "Iteration 175, loss = 2.61647356\n",
      "Iteration 176, loss = 2.60871181\n",
      "Iteration 177, loss = 2.60187252\n",
      "Iteration 178, loss = 2.59477743\n",
      "Iteration 179, loss = 2.58818636\n",
      "Iteration 180, loss = 2.58117469\n",
      "Iteration 181, loss = 2.57499695\n",
      "Iteration 182, loss = 2.56703835\n",
      "Iteration 183, loss = 2.56036439\n",
      "Iteration 184, loss = 2.55024148\n",
      "Iteration 185, loss = 2.53946083\n",
      "Iteration 186, loss = 2.52631759\n",
      "Iteration 187, loss = 2.50867363\n",
      "Iteration 188, loss = 2.48934871\n",
      "Iteration 189, loss = 2.46463238\n",
      "Iteration 190, loss = 2.43975824\n",
      "Iteration 191, loss = 2.41288225\n",
      "Iteration 192, loss = 2.38379442\n",
      "Iteration 193, loss = 2.36128043\n",
      "Iteration 194, loss = 2.33242638\n",
      "Iteration 195, loss = 2.30258087\n",
      "Iteration 196, loss = 2.27450469\n",
      "Iteration 197, loss = 2.24397250\n",
      "Iteration 198, loss = 2.21379245\n",
      "Iteration 199, loss = 2.18632989\n",
      "Iteration 200, loss = 2.15982676\n",
      "Iteration 201, loss = 2.13215103\n",
      "Iteration 202, loss = 2.10475866\n",
      "Iteration 203, loss = 2.07855708\n",
      "Iteration 204, loss = 2.05109190\n",
      "Iteration 205, loss = 2.02656502\n",
      "Iteration 206, loss = 1.99929168\n",
      "Iteration 207, loss = 1.97576469\n",
      "Iteration 208, loss = 1.94838384\n",
      "Iteration 209, loss = 1.92467345\n",
      "Iteration 210, loss = 1.90092810\n",
      "Iteration 211, loss = 1.87686463\n",
      "Iteration 212, loss = 1.85391504\n",
      "Iteration 213, loss = 1.83247086\n",
      "Iteration 214, loss = 1.81480240\n",
      "Iteration 215, loss = 1.78920380\n",
      "Iteration 216, loss = 1.76882006\n",
      "Iteration 217, loss = 1.74717423\n",
      "Iteration 218, loss = 1.72781495\n",
      "Iteration 219, loss = 1.70762249\n",
      "Iteration 220, loss = 1.68805078\n",
      "Iteration 221, loss = 1.66895333\n",
      "Iteration 222, loss = 1.65193749\n",
      "Iteration 223, loss = 1.63210956\n",
      "Iteration 224, loss = 1.61788718\n",
      "Iteration 225, loss = 1.59843677\n",
      "Iteration 226, loss = 1.58076615\n",
      "Iteration 227, loss = 1.56466596\n",
      "Iteration 228, loss = 1.54821988\n",
      "Iteration 229, loss = 1.53359905\n",
      "Iteration 230, loss = 1.51677682\n",
      "Iteration 231, loss = 1.50293802\n",
      "Iteration 232, loss = 1.48828291\n",
      "Iteration 233, loss = 1.47690196\n",
      "Iteration 234, loss = 1.45919663\n",
      "Iteration 235, loss = 1.44878119\n",
      "Iteration 236, loss = 1.43367658\n",
      "Iteration 237, loss = 1.42218711\n",
      "Iteration 238, loss = 1.40685830\n",
      "Iteration 239, loss = 1.39719258\n",
      "Iteration 240, loss = 1.37803785\n",
      "Iteration 241, loss = 1.35677243\n",
      "Iteration 242, loss = 1.34474129\n",
      "Iteration 243, loss = 1.32097867\n",
      "Iteration 244, loss = 1.30460531\n",
      "Iteration 245, loss = 1.28640771\n",
      "Iteration 246, loss = 1.26342733\n",
      "Iteration 247, loss = 1.25091256\n",
      "Iteration 248, loss = 1.23178520\n",
      "Iteration 249, loss = 1.20325223\n",
      "Iteration 250, loss = 1.19235004\n",
      "Iteration 251, loss = 1.16315636\n",
      "Iteration 252, loss = 1.15118607\n",
      "Iteration 253, loss = 1.13723355\n",
      "Iteration 254, loss = 1.11175950\n",
      "Iteration 255, loss = 1.09625651\n",
      "Iteration 256, loss = 1.08087167\n",
      "Iteration 257, loss = 1.06474105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 1.04469225\n",
      "Iteration 259, loss = 1.02933057\n",
      "Iteration 260, loss = 1.01333456\n",
      "Iteration 261, loss = 1.00221203\n",
      "Iteration 262, loss = 0.98190671\n",
      "Iteration 263, loss = 0.97042574\n",
      "Iteration 264, loss = 0.95989976\n",
      "Iteration 265, loss = 0.93983796\n",
      "Iteration 266, loss = 0.92877028\n",
      "Iteration 267, loss = 0.91821207\n",
      "Iteration 268, loss = 0.90028760\n",
      "Iteration 269, loss = 0.89138502\n",
      "Iteration 270, loss = 0.88034115\n",
      "Iteration 271, loss = 0.86462060\n",
      "Iteration 272, loss = 0.85487165\n",
      "Iteration 273, loss = 0.84582771\n",
      "Iteration 274, loss = 0.83254164\n",
      "Iteration 275, loss = 0.82175496\n",
      "Iteration 276, loss = 0.81182493\n",
      "Iteration 277, loss = 0.80159793\n",
      "Iteration 278, loss = 0.78980413\n",
      "Iteration 279, loss = 0.77993303\n",
      "Iteration 280, loss = 0.77136489\n",
      "Iteration 281, loss = 0.76195766\n",
      "Iteration 282, loss = 0.75519756\n",
      "Iteration 283, loss = 0.74165022\n",
      "Iteration 284, loss = 0.72761614\n",
      "Iteration 285, loss = 0.71976817\n",
      "Iteration 286, loss = 0.71056044\n",
      "Iteration 287, loss = 0.70151773\n",
      "Iteration 288, loss = 0.68933615\n",
      "Iteration 289, loss = 0.69068203\n",
      "Iteration 290, loss = 0.67269273\n",
      "Iteration 291, loss = 0.66252946\n",
      "Iteration 292, loss = 0.65159072\n",
      "Iteration 293, loss = 0.64854128\n",
      "Iteration 294, loss = 0.63340037\n",
      "Iteration 295, loss = 0.62427096\n",
      "Iteration 296, loss = 0.61419646\n",
      "Iteration 297, loss = 0.60710552\n",
      "Iteration 298, loss = 0.59893977\n",
      "Iteration 299, loss = 0.59042680\n",
      "Iteration 300, loss = 0.58350175\n",
      "Iteration 301, loss = 0.57231948\n",
      "Iteration 302, loss = 0.56192785\n",
      "Iteration 303, loss = 0.55692277\n",
      "Iteration 304, loss = 0.54886803\n",
      "Iteration 305, loss = 0.54038658\n",
      "Iteration 306, loss = 0.53561240\n",
      "Iteration 307, loss = 0.53033082\n",
      "Iteration 308, loss = 0.51897079\n",
      "Iteration 309, loss = 0.51679546\n",
      "Iteration 310, loss = 0.50445295\n",
      "Iteration 311, loss = 0.50073223\n",
      "Iteration 312, loss = 0.49320654\n",
      "Iteration 313, loss = 0.48678381\n",
      "Iteration 314, loss = 0.48265423\n",
      "Iteration 315, loss = 0.47454374\n",
      "Iteration 316, loss = 0.46931326\n",
      "Iteration 317, loss = 0.46400814\n",
      "Iteration 318, loss = 0.46012374\n",
      "Iteration 319, loss = 0.45410725\n",
      "Iteration 320, loss = 0.44783003\n",
      "Iteration 321, loss = 0.44233118\n",
      "Iteration 322, loss = 0.43744396\n",
      "Iteration 323, loss = 0.43218173\n",
      "Iteration 324, loss = 0.42829086\n",
      "Iteration 325, loss = 0.42287149\n",
      "Iteration 326, loss = 0.41773294\n",
      "Iteration 327, loss = 0.41215144\n",
      "Iteration 328, loss = 0.40863824\n",
      "Iteration 329, loss = 0.40454322\n",
      "Iteration 330, loss = 0.39819885\n",
      "Iteration 331, loss = 0.39330054\n",
      "Iteration 332, loss = 0.38865585\n",
      "Iteration 333, loss = 0.38531471\n",
      "Iteration 334, loss = 0.37958868\n",
      "Iteration 335, loss = 0.37557592\n",
      "Iteration 336, loss = 0.37154545\n",
      "Iteration 337, loss = 0.36770201\n",
      "Iteration 338, loss = 0.36328243\n",
      "Iteration 339, loss = 0.35913031\n",
      "Iteration 340, loss = 0.35443662\n",
      "Iteration 341, loss = 0.35079624\n",
      "Iteration 342, loss = 0.34692334\n",
      "Iteration 343, loss = 0.34360051\n",
      "Iteration 344, loss = 0.34040961\n",
      "Iteration 345, loss = 0.33653013\n",
      "Iteration 346, loss = 0.33245569\n",
      "Iteration 347, loss = 0.32976850\n",
      "Iteration 348, loss = 0.32622698\n",
      "Iteration 349, loss = 0.32308386\n",
      "Iteration 350, loss = 0.31951703\n",
      "Iteration 351, loss = 0.31698628\n",
      "Iteration 352, loss = 0.31458217\n",
      "Iteration 353, loss = 0.31161428\n",
      "Iteration 354, loss = 0.30832825\n",
      "Iteration 355, loss = 0.30751656\n",
      "Iteration 356, loss = 0.30377599\n",
      "Iteration 357, loss = 0.30184394\n",
      "Iteration 358, loss = 0.29799960\n",
      "Iteration 359, loss = 0.29612635\n",
      "Iteration 360, loss = 0.29291478\n",
      "Iteration 361, loss = 0.29179517\n",
      "Iteration 362, loss = 0.28734446\n",
      "Iteration 363, loss = 0.28709193\n",
      "Iteration 364, loss = 0.28384376\n",
      "Iteration 365, loss = 0.28283729\n",
      "Iteration 366, loss = 0.27956436\n",
      "Iteration 367, loss = 0.27737993\n",
      "Iteration 368, loss = 0.27518016\n",
      "Iteration 369, loss = 0.27412973\n",
      "Iteration 370, loss = 0.27142441\n",
      "Iteration 371, loss = 0.26936528\n",
      "Iteration 372, loss = 0.26716776\n",
      "Iteration 373, loss = 0.26705672\n",
      "Iteration 374, loss = 0.26498201\n",
      "Iteration 375, loss = 0.26269572\n",
      "Iteration 376, loss = 0.26026881\n",
      "Iteration 377, loss = 0.25870508\n",
      "Iteration 378, loss = 0.25660663\n",
      "Iteration 379, loss = 0.25480878\n",
      "Iteration 380, loss = 0.25341089\n",
      "Iteration 381, loss = 0.25144440\n",
      "Iteration 382, loss = 0.24989466\n",
      "Iteration 383, loss = 0.24865107\n",
      "Iteration 384, loss = 0.24676886\n",
      "Iteration 385, loss = 0.24536191\n",
      "Iteration 386, loss = 0.24463272\n",
      "Iteration 387, loss = 0.24320010\n",
      "Iteration 388, loss = 0.24084023\n",
      "Iteration 389, loss = 0.23969456\n",
      "Iteration 390, loss = 0.23795664\n",
      "Iteration 391, loss = 0.23702088\n",
      "Iteration 392, loss = 0.23538892\n",
      "Iteration 393, loss = 0.23411268\n",
      "Iteration 394, loss = 0.23360569\n",
      "Iteration 395, loss = 0.23164515\n",
      "Iteration 396, loss = 0.23034039\n",
      "Iteration 397, loss = 0.22887575\n",
      "Iteration 398, loss = 0.22798845\n",
      "Iteration 399, loss = 0.22815072\n",
      "Iteration 400, loss = 0.22551025\n",
      "Iteration 401, loss = 0.22436489\n",
      "Iteration 402, loss = 0.22328826\n",
      "Iteration 403, loss = 0.22159354\n",
      "Iteration 404, loss = 0.22116664\n",
      "Iteration 405, loss = 0.21936505\n",
      "Iteration 406, loss = 0.21881364\n",
      "Iteration 407, loss = 0.21784082\n",
      "Iteration 408, loss = 0.21677807\n",
      "Iteration 409, loss = 0.21540199\n",
      "Iteration 410, loss = 0.21463489\n",
      "Iteration 411, loss = 0.21370065\n",
      "Iteration 412, loss = 0.21245394\n",
      "Iteration 413, loss = 0.21089031\n",
      "Iteration 414, loss = 0.21134243\n",
      "Iteration 415, loss = 0.21113478\n",
      "Iteration 416, loss = 0.20860962\n",
      "Iteration 417, loss = 0.21059468\n",
      "Iteration 418, loss = 0.20828995\n",
      "Iteration 419, loss = 0.20600672\n",
      "Iteration 420, loss = 0.20669775\n",
      "Iteration 421, loss = 0.20327976\n",
      "Iteration 422, loss = 0.20440575\n",
      "Iteration 423, loss = 0.20178755\n",
      "Iteration 424, loss = 0.20078982\n",
      "Iteration 425, loss = 0.20020899\n",
      "Iteration 426, loss = 0.19960666\n",
      "Iteration 427, loss = 0.19855720\n",
      "Iteration 428, loss = 0.19771592\n",
      "Iteration 429, loss = 0.19668489\n",
      "Iteration 430, loss = 0.19584223\n",
      "Iteration 431, loss = 0.19506463\n",
      "Iteration 432, loss = 0.19445346\n",
      "Iteration 433, loss = 0.19298033\n",
      "Iteration 434, loss = 0.19313037\n",
      "Iteration 435, loss = 0.19169647\n",
      "Iteration 436, loss = 0.19091405\n",
      "Iteration 437, loss = 0.19045216\n",
      "Iteration 438, loss = 0.18949710\n",
      "Iteration 439, loss = 0.18900063\n",
      "Iteration 440, loss = 0.18834710\n",
      "Iteration 441, loss = 0.18791992\n",
      "Iteration 442, loss = 0.18707455\n",
      "Iteration 443, loss = 0.18584538\n",
      "Iteration 444, loss = 0.18553898\n",
      "Iteration 445, loss = 0.18488902\n",
      "Iteration 446, loss = 0.18405350\n",
      "Iteration 447, loss = 0.18347154\n",
      "Iteration 448, loss = 0.18258825\n",
      "Iteration 449, loss = 0.18260889\n",
      "Iteration 450, loss = 0.18170982\n",
      "Iteration 451, loss = 0.18074438\n",
      "Iteration 452, loss = 0.18053041\n",
      "Iteration 453, loss = 0.18021044\n",
      "Iteration 454, loss = 0.18018525\n",
      "Iteration 455, loss = 0.17829451\n",
      "Iteration 456, loss = 0.17816977\n",
      "Iteration 457, loss = 0.17955736\n",
      "Iteration 458, loss = 0.17737592\n",
      "Iteration 459, loss = 0.17645277\n",
      "Iteration 460, loss = 0.17606551\n",
      "Iteration 461, loss = 0.17535762\n",
      "Iteration 462, loss = 0.17477200\n",
      "Iteration 463, loss = 0.17396927\n",
      "Iteration 464, loss = 0.17360433\n",
      "Iteration 465, loss = 0.17296354\n",
      "Iteration 466, loss = 0.17241138\n",
      "Iteration 467, loss = 0.17193023\n",
      "Iteration 468, loss = 0.17125727\n",
      "Iteration 469, loss = 0.17086350\n",
      "Iteration 470, loss = 0.16978297\n",
      "Iteration 471, loss = 0.16985817\n",
      "Iteration 472, loss = 0.16866408\n",
      "Iteration 473, loss = 0.16866488\n",
      "Iteration 474, loss = 0.16794348\n",
      "Iteration 475, loss = 0.16926153\n",
      "Iteration 476, loss = 0.16686273\n",
      "Iteration 477, loss = 0.16812329\n",
      "Iteration 478, loss = 0.16614198\n",
      "Iteration 479, loss = 0.16622584\n",
      "Iteration 480, loss = 0.16574191\n",
      "Iteration 481, loss = 0.16480722\n",
      "Iteration 482, loss = 0.16475744\n",
      "Iteration 483, loss = 0.16351327\n",
      "Iteration 484, loss = 0.16333579\n",
      "Iteration 485, loss = 0.16252718\n",
      "Iteration 486, loss = 0.16254944\n",
      "Iteration 487, loss = 0.16433920\n",
      "Iteration 488, loss = 0.16233544\n",
      "Iteration 489, loss = 0.16154201\n",
      "Iteration 490, loss = 0.16167676\n",
      "Iteration 491, loss = 0.15962012\n",
      "Iteration 492, loss = 0.16133852\n",
      "Iteration 493, loss = 0.16089621\n",
      "Iteration 494, loss = 0.16011029\n",
      "Iteration 495, loss = 0.15937527\n",
      "Iteration 496, loss = 0.15804872\n",
      "Iteration 497, loss = 0.15733216\n",
      "Iteration 498, loss = 0.15698071\n",
      "Iteration 499, loss = 0.15741530\n",
      "Iteration 500, loss = 0.15777753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train) # usa modelo red neuronal que hemos definido antes y(x,w)\n",
    "# mlp.loss_curve en Clasificación te guarda el valor de la función coste a cada iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en python2.7 no funciona para regresión, en python 3 si. Así podemos ver como avanza\n",
    "#pl.plot(mlp.loss_curve_)  # mlp.loss_curve nos da una idea de como converge el training set\n",
    "#pl.xlabel('iteration')\n",
    "#pl.ylabel('loss function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficiente correlación 0.9999296989957829\n",
      "error relativo 0.3254770692986282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x120ce4a10>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test) # estimamos resultado con el TEST\n",
    "print 'coeficiente correlación', np.corrcoef(predictions,y_test)[0][1]  # calculamos correlación con el valor esperado\n",
    "print 'error relativo', np.mean(abs(predictions-y_test)/y_test)*100  # calculamos correlación con el valor esperado\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "pl.plot(predictions,y_test,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le pregunto a la red que me sume 10+30+5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma predicha es 9.0\n",
      "la suma real es 9.0\n"
     ]
    }
   ],
   "source": [
    "A=np.array([1,2,6]).reshape(1,-1) # sklearn necesita que se lo de dimension(1,n)\n",
    "print 'la suma predicha es', round(mlp.predict(A)) # redondamos\n",
    "print 'la suma real es',round(np.sum(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma predicha es 3541.0\n",
      "la suma real es 3550.0\n"
     ]
    }
   ],
   "source": [
    "# si miramos valores muy lejos del rango\n",
    "A=np.array([1050,1250,1250]).reshape(1,-1) \n",
    "np.shape(A)\n",
    "print 'la suma predicha es', round(mlp.predict(A)) # redondamos\n",
    "print 'la suma real es',round(np.sum(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO ha aprendido a sumar. Ha ajustado muy bien los datos y un entorno relativamente ancho\n"
     ]
    }
   ],
   "source": [
    "print 'NO ha aprendido a sumar. Ha ajustado muy bien los datos y un entorno relativamente ancho'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
