{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a entrenar una red para que sume 3 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#training set, en este caso lo simulamos. Generamos 3 números aleatorios X1, X2 y X3 (inputs) y los sumamos (Y, output) \n",
    "X1=np.random.uniform(size=1000)*100\n",
    "X2=np.random.uniform(size=1000)*100\n",
    "X3=np.random.uniform(size=1000)*100\n",
    "X=np.transpose([X1,X2,X3])\n",
    "Y=X1+X2+X3\n",
    "print np.shape(Y),np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y) \n",
    "# separa el set total entre train y test (3/4 y 1/4 por default) . Exactamente lo mismo que hacer:\n",
    "#ntrain=3*len(y)/4\n",
    "#X_train=X[:ntrain,:];y_train=y[:ntrain]\n",
    "#X_test=X[ntrain:,:];y_test=y[ntrain:]\n",
    "np.shape(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# definimos numero de capas y de hidden nodes (3 capas, de 2 ,5  y 10 neuronas) \n",
    "# Exagerado para este problema ..\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(5),max_iter=500,verbose=True) # batch_size default depending on minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12836922\n",
      "Iteration 2, loss = 1.10245035\n",
      "Iteration 3, loss = 1.07742459\n",
      "Iteration 4, loss = 1.05252621\n",
      "Iteration 5, loss = 1.02846487\n",
      "Iteration 6, loss = 1.00461687\n",
      "Iteration 7, loss = 0.98168306\n",
      "Iteration 8, loss = 0.95953497\n",
      "Iteration 9, loss = 0.93790694\n",
      "Iteration 10, loss = 0.91731165\n",
      "Iteration 11, loss = 0.89663168\n",
      "Iteration 12, loss = 0.87681237\n",
      "Iteration 13, loss = 0.85798119\n",
      "Iteration 14, loss = 0.83882918\n",
      "Iteration 15, loss = 0.82074194\n",
      "Iteration 16, loss = 0.80273215\n",
      "Iteration 17, loss = 0.78536300\n",
      "Iteration 18, loss = 0.76843408\n",
      "Iteration 19, loss = 0.75141059\n",
      "Iteration 20, loss = 0.73536763\n",
      "Iteration 21, loss = 0.71867358\n",
      "Iteration 22, loss = 0.70257566\n",
      "Iteration 23, loss = 0.68647944\n",
      "Iteration 24, loss = 0.67103764\n",
      "Iteration 25, loss = 0.65477745\n",
      "Iteration 26, loss = 0.63917271\n",
      "Iteration 27, loss = 0.62393870\n",
      "Iteration 28, loss = 0.60873687\n",
      "Iteration 29, loss = 0.59374312\n",
      "Iteration 30, loss = 0.57925952\n",
      "Iteration 31, loss = 0.56526714\n",
      "Iteration 32, loss = 0.55093118\n",
      "Iteration 33, loss = 0.53668493\n",
      "Iteration 34, loss = 0.52296448\n",
      "Iteration 35, loss = 0.50915225\n",
      "Iteration 36, loss = 0.49590323\n",
      "Iteration 37, loss = 0.48247666\n",
      "Iteration 38, loss = 0.46974681\n",
      "Iteration 39, loss = 0.45727116\n",
      "Iteration 40, loss = 0.44466232\n",
      "Iteration 41, loss = 0.43249474\n",
      "Iteration 42, loss = 0.42096966\n",
      "Iteration 43, loss = 0.40946902\n",
      "Iteration 44, loss = 0.39853073\n",
      "Iteration 45, loss = 0.38801182\n",
      "Iteration 46, loss = 0.37800236\n",
      "Iteration 47, loss = 0.36788283\n",
      "Iteration 48, loss = 0.35872228\n",
      "Iteration 49, loss = 0.34972093\n",
      "Iteration 50, loss = 0.34100952\n",
      "Iteration 51, loss = 0.33267446\n",
      "Iteration 52, loss = 0.32438246\n",
      "Iteration 53, loss = 0.31622268\n",
      "Iteration 54, loss = 0.30872894\n",
      "Iteration 55, loss = 0.30113608\n",
      "Iteration 56, loss = 0.29411612\n",
      "Iteration 57, loss = 0.28691604\n",
      "Iteration 58, loss = 0.28013309\n",
      "Iteration 59, loss = 0.27376251\n",
      "Iteration 60, loss = 0.26713380\n",
      "Iteration 61, loss = 0.26113866\n",
      "Iteration 62, loss = 0.25527791\n",
      "Iteration 63, loss = 0.24959920\n",
      "Iteration 64, loss = 0.24424885\n",
      "Iteration 65, loss = 0.23880921\n",
      "Iteration 66, loss = 0.23379756\n",
      "Iteration 67, loss = 0.22876505\n",
      "Iteration 68, loss = 0.22393851\n",
      "Iteration 69, loss = 0.21937700\n",
      "Iteration 70, loss = 0.21483272\n",
      "Iteration 71, loss = 0.21031526\n",
      "Iteration 72, loss = 0.20611579\n",
      "Iteration 73, loss = 0.20197110\n",
      "Iteration 74, loss = 0.19789674\n",
      "Iteration 75, loss = 0.19406741\n",
      "Iteration 76, loss = 0.19025044\n",
      "Iteration 77, loss = 0.18651468\n",
      "Iteration 78, loss = 0.18309136\n",
      "Iteration 79, loss = 0.17944697\n",
      "Iteration 80, loss = 0.17611441\n",
      "Iteration 81, loss = 0.17279966\n",
      "Iteration 82, loss = 0.16952577\n",
      "Iteration 83, loss = 0.16646529\n",
      "Iteration 84, loss = 0.16334452\n",
      "Iteration 85, loss = 0.16040373\n",
      "Iteration 86, loss = 0.15752338\n",
      "Iteration 87, loss = 0.15463884\n",
      "Iteration 88, loss = 0.15197287\n",
      "Iteration 89, loss = 0.14931004\n",
      "Iteration 90, loss = 0.14674905\n",
      "Iteration 91, loss = 0.14416098\n",
      "Iteration 92, loss = 0.14185329\n",
      "Iteration 93, loss = 0.13940228\n",
      "Iteration 94, loss = 0.13710526\n",
      "Iteration 95, loss = 0.13487648\n",
      "Iteration 96, loss = 0.13262640\n",
      "Iteration 97, loss = 0.13049694\n",
      "Iteration 98, loss = 0.12832394\n",
      "Iteration 99, loss = 0.12631531\n",
      "Iteration 100, loss = 0.12427731\n",
      "Iteration 101, loss = 0.12234346\n",
      "Iteration 102, loss = 0.12030669\n",
      "Iteration 103, loss = 0.11840941\n",
      "Iteration 104, loss = 0.11660778\n",
      "Iteration 105, loss = 0.11480264\n",
      "Iteration 106, loss = 0.11297112\n",
      "Iteration 107, loss = 0.11120359\n",
      "Iteration 108, loss = 0.10950756\n",
      "Iteration 109, loss = 0.10782191\n",
      "Iteration 110, loss = 0.10626654\n",
      "Iteration 111, loss = 0.10461079\n",
      "Iteration 112, loss = 0.10312559\n",
      "Iteration 113, loss = 0.10153413\n",
      "Iteration 114, loss = 0.10010077\n",
      "Iteration 115, loss = 0.09865238\n",
      "Iteration 116, loss = 0.09727851\n",
      "Iteration 117, loss = 0.09580627\n",
      "Iteration 118, loss = 0.09448563\n",
      "Iteration 119, loss = 0.09314020\n",
      "Iteration 120, loss = 0.09186017\n",
      "Iteration 121, loss = 0.09057327\n",
      "Iteration 122, loss = 0.08928982\n",
      "Iteration 123, loss = 0.08806123\n",
      "Iteration 124, loss = 0.08683986\n",
      "Iteration 125, loss = 0.08561777\n",
      "Iteration 126, loss = 0.08443802\n",
      "Iteration 127, loss = 0.08332756\n",
      "Iteration 128, loss = 0.08219129\n",
      "Iteration 129, loss = 0.08109419\n",
      "Iteration 130, loss = 0.08004482\n",
      "Iteration 131, loss = 0.07897591\n",
      "Iteration 132, loss = 0.07799672\n",
      "Iteration 133, loss = 0.07701309\n",
      "Iteration 134, loss = 0.07604059\n",
      "Iteration 135, loss = 0.07507390\n",
      "Iteration 136, loss = 0.07418630\n",
      "Iteration 137, loss = 0.07326278\n",
      "Iteration 138, loss = 0.07240068\n",
      "Iteration 139, loss = 0.07151218\n",
      "Iteration 140, loss = 0.07065174\n",
      "Iteration 141, loss = 0.06984721\n",
      "Iteration 142, loss = 0.06900060\n",
      "Iteration 143, loss = 0.06816806\n",
      "Iteration 144, loss = 0.06743140\n",
      "Iteration 145, loss = 0.06665639\n",
      "Iteration 146, loss = 0.06587197\n",
      "Iteration 147, loss = 0.06513780\n",
      "Iteration 148, loss = 0.06441522\n",
      "Iteration 149, loss = 0.06368250\n",
      "Iteration 150, loss = 0.06304506\n",
      "Iteration 151, loss = 0.06227924\n",
      "Iteration 152, loss = 0.06161656\n",
      "Iteration 153, loss = 0.06094979\n",
      "Iteration 154, loss = 0.06026079\n",
      "Iteration 155, loss = 0.05961953\n",
      "Iteration 156, loss = 0.05897741\n",
      "Iteration 157, loss = 0.05831929\n",
      "Iteration 158, loss = 0.05772361\n",
      "Iteration 159, loss = 0.05707996\n",
      "Iteration 160, loss = 0.05644784\n",
      "Iteration 161, loss = 0.05583814\n",
      "Iteration 162, loss = 0.05524969\n",
      "Iteration 163, loss = 0.05465408\n",
      "Iteration 164, loss = 0.05406033\n",
      "Iteration 165, loss = 0.05345663\n",
      "Iteration 166, loss = 0.05286734\n",
      "Iteration 167, loss = 0.05230212\n",
      "Iteration 168, loss = 0.05171378\n",
      "Iteration 169, loss = 0.05119110\n",
      "Iteration 170, loss = 0.05061766\n",
      "Iteration 171, loss = 0.05008314\n",
      "Iteration 172, loss = 0.04956166\n",
      "Iteration 173, loss = 0.04901724\n",
      "Iteration 174, loss = 0.04848485\n",
      "Iteration 175, loss = 0.04796226\n",
      "Iteration 176, loss = 0.04744855\n",
      "Iteration 177, loss = 0.04694878\n",
      "Iteration 178, loss = 0.04643300\n",
      "Iteration 179, loss = 0.04593566\n",
      "Iteration 180, loss = 0.04544047\n",
      "Iteration 181, loss = 0.04494888\n",
      "Iteration 182, loss = 0.04448348\n",
      "Iteration 183, loss = 0.04401343\n",
      "Iteration 184, loss = 0.04355177\n",
      "Iteration 185, loss = 0.04308815\n",
      "Iteration 186, loss = 0.04263494\n",
      "Iteration 187, loss = 0.04217651\n",
      "Iteration 188, loss = 0.04174684\n",
      "Iteration 189, loss = 0.04131112\n",
      "Iteration 190, loss = 0.04086234\n",
      "Iteration 191, loss = 0.04042089\n",
      "Iteration 192, loss = 0.04000625\n",
      "Iteration 193, loss = 0.03958734\n",
      "Iteration 194, loss = 0.03917829\n",
      "Iteration 195, loss = 0.03875919\n",
      "Iteration 196, loss = 0.03836621\n",
      "Iteration 197, loss = 0.03794684\n",
      "Iteration 198, loss = 0.03755539\n",
      "Iteration 199, loss = 0.03716372\n",
      "Iteration 200, loss = 0.03675832\n",
      "Iteration 201, loss = 0.03637755\n",
      "Iteration 202, loss = 0.03597893\n",
      "Iteration 203, loss = 0.03557524\n",
      "Iteration 204, loss = 0.03519815\n",
      "Iteration 205, loss = 0.03480722\n",
      "Iteration 206, loss = 0.03439923\n",
      "Iteration 207, loss = 0.03402200\n",
      "Iteration 208, loss = 0.03362993\n",
      "Iteration 209, loss = 0.03326196\n",
      "Iteration 210, loss = 0.03289450\n",
      "Iteration 211, loss = 0.03252399\n",
      "Iteration 212, loss = 0.03215288\n",
      "Iteration 213, loss = 0.03181082\n",
      "Iteration 214, loss = 0.03145297\n",
      "Iteration 215, loss = 0.03110665\n",
      "Iteration 216, loss = 0.03076508\n",
      "Iteration 217, loss = 0.03042148\n",
      "Iteration 218, loss = 0.03009289\n",
      "Iteration 219, loss = 0.02975888\n",
      "Iteration 220, loss = 0.02943767\n",
      "Iteration 221, loss = 0.02911170\n",
      "Iteration 222, loss = 0.02878061\n",
      "Iteration 223, loss = 0.02846388\n",
      "Iteration 224, loss = 0.02813153\n",
      "Iteration 225, loss = 0.02780554\n",
      "Iteration 226, loss = 0.02749928\n",
      "Iteration 227, loss = 0.02718547\n",
      "Iteration 228, loss = 0.02686859\n",
      "Iteration 229, loss = 0.02655786\n",
      "Iteration 230, loss = 0.02624408\n",
      "Iteration 231, loss = 0.02594795\n",
      "Iteration 232, loss = 0.02564792\n",
      "Iteration 233, loss = 0.02535846\n",
      "Iteration 234, loss = 0.02506797\n",
      "Iteration 235, loss = 0.02478251\n",
      "Iteration 236, loss = 0.02450383\n",
      "Iteration 237, loss = 0.02421589\n",
      "Iteration 238, loss = 0.02393628\n",
      "Iteration 239, loss = 0.02366431\n",
      "Iteration 240, loss = 0.02338230\n",
      "Iteration 241, loss = 0.02312299\n",
      "Iteration 242, loss = 0.02285753\n",
      "Iteration 243, loss = 0.02259417\n",
      "Iteration 244, loss = 0.02233614\n",
      "Iteration 245, loss = 0.02208870\n",
      "Iteration 246, loss = 0.02183118\n",
      "Iteration 247, loss = 0.02158067\n",
      "Iteration 248, loss = 0.02134052\n",
      "Iteration 249, loss = 0.02109608\n",
      "Iteration 250, loss = 0.02086011\n",
      "Iteration 251, loss = 0.02062803\n",
      "Iteration 252, loss = 0.02039204\n",
      "Iteration 253, loss = 0.02016973\n",
      "Iteration 254, loss = 0.01993915\n",
      "Iteration 255, loss = 0.01972058\n",
      "Iteration 256, loss = 0.01950209\n",
      "Iteration 257, loss = 0.01929205\n",
      "Iteration 258, loss = 0.01907305\n",
      "Iteration 259, loss = 0.01886833\n",
      "Iteration 260, loss = 0.01866330\n",
      "Iteration 261, loss = 0.01846378\n",
      "Iteration 262, loss = 0.01825732\n",
      "Iteration 263, loss = 0.01805725\n",
      "Iteration 264, loss = 0.01786176\n",
      "Iteration 265, loss = 0.01767181\n",
      "Iteration 266, loss = 0.01746952\n",
      "Iteration 267, loss = 0.01728367\n",
      "Iteration 268, loss = 0.01709428\n",
      "Iteration 269, loss = 0.01690260\n",
      "Iteration 270, loss = 0.01671748\n",
      "Iteration 271, loss = 0.01653669\n",
      "Iteration 272, loss = 0.01636139\n",
      "Iteration 273, loss = 0.01618328\n",
      "Iteration 274, loss = 0.01599632\n",
      "Iteration 275, loss = 0.01582654\n",
      "Iteration 276, loss = 0.01565937\n",
      "Iteration 277, loss = 0.01548822\n",
      "Iteration 278, loss = 0.01532065\n",
      "Iteration 279, loss = 0.01515960\n",
      "Iteration 280, loss = 0.01499987\n",
      "Iteration 281, loss = 0.01483406\n",
      "Iteration 282, loss = 0.01467070\n",
      "Iteration 283, loss = 0.01451076\n",
      "Iteration 284, loss = 0.01435459\n",
      "Iteration 285, loss = 0.01419509\n",
      "Iteration 286, loss = 0.01404319\n",
      "Iteration 287, loss = 0.01388800\n",
      "Iteration 288, loss = 0.01373581\n",
      "Iteration 289, loss = 0.01358427\n",
      "Iteration 290, loss = 0.01343489\n",
      "Iteration 291, loss = 0.01329050\n",
      "Iteration 292, loss = 0.01314382\n",
      "Iteration 293, loss = 0.01299772\n",
      "Iteration 294, loss = 0.01285122\n",
      "Iteration 295, loss = 0.01271336\n",
      "Iteration 296, loss = 0.01256742\n",
      "Iteration 297, loss = 0.01243008\n",
      "Iteration 298, loss = 0.01229273\n",
      "Iteration 299, loss = 0.01215389\n",
      "Iteration 300, loss = 0.01201980\n",
      "Iteration 301, loss = 0.01189365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 302, loss = 0.01176473\n",
      "Iteration 303, loss = 0.01163425\n",
      "Iteration 304, loss = 0.01150553\n",
      "Iteration 305, loss = 0.01138411\n",
      "Iteration 306, loss = 0.01126155\n",
      "Iteration 307, loss = 0.01113670\n",
      "Iteration 308, loss = 0.01101858\n",
      "Iteration 309, loss = 0.01090228\n",
      "Iteration 310, loss = 0.01078243\n",
      "Iteration 311, loss = 0.01066782\n",
      "Iteration 312, loss = 0.01055685\n",
      "Iteration 313, loss = 0.01044503\n",
      "Iteration 314, loss = 0.01033343\n",
      "Iteration 315, loss = 0.01022702\n",
      "Iteration 316, loss = 0.01011919\n",
      "Iteration 317, loss = 0.01001322\n",
      "Iteration 318, loss = 0.00990820\n",
      "Iteration 319, loss = 0.00980227\n",
      "Iteration 320, loss = 0.00970337\n",
      "Iteration 321, loss = 0.00959917\n",
      "Iteration 322, loss = 0.00949712\n",
      "Iteration 323, loss = 0.00940186\n",
      "Iteration 324, loss = 0.00930055\n",
      "Iteration 325, loss = 0.00920623\n",
      "Iteration 326, loss = 0.00911022\n",
      "Iteration 327, loss = 0.00901759\n",
      "Iteration 328, loss = 0.00892214\n",
      "Iteration 329, loss = 0.00883061\n",
      "Iteration 330, loss = 0.00874178\n",
      "Iteration 331, loss = 0.00865754\n",
      "Iteration 332, loss = 0.00856610\n",
      "Iteration 333, loss = 0.00848381\n",
      "Iteration 334, loss = 0.00839630\n",
      "Iteration 335, loss = 0.00831241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=5, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train) # usa modelo red neuronal que hemos definido antes y(x,w)\n",
    "# mlp.loss_curve en Clasificación te guarda el valor de la función coste a cada iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9360299247088049\n",
      "53.37306166950245\n",
      "[[1.         0.99114194]\n",
      " [0.99114194 1.        ]]\n",
      "148.2278403738784 48.99447962750003\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test) # estimamos resultado con el TEST\n",
    "print np.corrcoef(predictions,y_test)  # calculamos correlación con el valor esperado\n",
    "import matplotlib.pyplot as pl\n",
    "pl.plot(predictions,y_test,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le pregunto a la red que me sume 10+30+5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma es 601.0\n"
     ]
    }
   ],
   "source": [
    "A=np.array([12,30,5]).reshape(1,-1) # sklearn necesita que se lo de dimension(1,n)\n",
    "print 'la suma es', round(mlp.predict(A)) # redondamos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma es 1294.0\n"
     ]
    }
   ],
   "source": [
    "# si miramos valores muy lejos del rango\n",
    "A=np.array([250,1000,2001]).reshape(1,-1) \n",
    "np.shape(A)\n",
    "print 'la suma es', round(mlp.predict(A)) # redondamos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO ha aprendido a sumar. Ha ajustado muy bien los datos y un entorno relativamente ancho\n"
     ]
    }
   ],
   "source": [
    "print 'NO ha aprendido a sumar. Ha ajustado muy bien los datos y un entorno relativamente ancho'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
